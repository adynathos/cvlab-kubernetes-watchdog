{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes_asyncio import client, config\n",
    "import kubernetes_asyncio as kube\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedn bedn2 bedn3 benlalahpod.com guillard-sinkhorn-gridnet katircio-imagenet katircio-imagenet2 kicirogl-airsim-1 kicirogl-airsim-2 mv-cvpr3 nakka-advseg11 nakka-advseg14 nakka-advseg20 nsd oner-rt-fulldata-40 oner-rt-fulldata-40-2 rbermude-multiflow remelli-mva remelli-mvp sguo-pytorch-imagenet-iclr vidit-maskrcnn-seg-res50-2 wickrama-experiments-1 wickrama-experiments-2 wickrama-experiments-3 wwang-imagenet0 wwang-imagenet1 wwang-imagenet2\n"
     ]
    }
   ],
   "source": [
    "async def list_pods_once():\n",
    "\tawait kube.config.load_kube_config()\n",
    "\tapi = kube.client.CoreV1Api()\n",
    "\t\n",
    "\tpod_list_response = await api.list_namespaced_pod('cvlab')\n",
    "\t\n",
    "\tprint(' '.join(f'{pod.metadata.name}' for pod in pod_list_response.items))\n",
    "\t\n",
    "\treturn pod_list_response\n",
    "\t\n",
    "# asyncio.run(main())\n",
    "pod_list = await list_pods_once()\n",
    "\n",
    "# Configs can be set in Configuration class directly or using helper\n",
    "# utility. If no argument provided, the config will be loaded from\n",
    "# default location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on pod [bedn]\n"
     ]
    }
   ],
   "source": [
    "pod_to_test = pod_list.items[0]\n",
    "print(f'Testing on pod [{pod_to_test.metadata.name}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU status with nvidia-smi\n",
    "\n",
    "Example queries: <https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries>  \n",
    "List of fields: <https://briot-jerome.developpez.com/fichiers/blog/nvidia-smi/list.txt>\n",
    "\n",
    "`nvidia-smi --format=csv --loop=2 --query-gpu=index,utilization.gpu,utilization.memory,memory.used,memory.total`\n",
    "\n",
    "Injections:  \n",
    "`kubectl exec -it container -- /usr/bin/nvidia-smi --format=csv --loop=2 --query-gpu=index,utilization.gpu,utilization.memory,memory.used,memory.total`\n",
    "\n",
    "Run command for 10s:  \n",
    "`timeout 10 something`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_get_namespaced_pod_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "connect_get_namespaced_pod_exec  # noqa: E501\n",
       "\n",
       "connect GET requests to exec of Pod  # noqa: E501\n",
       "This method makes a synchronous HTTP request by default. To make an\n",
       "asynchronous HTTP request, please pass async_req=True\n",
       ">>> thread = api.connect_get_namespaced_pod_exec(name, namespace, async_req=True)\n",
       ">>> result = thread.get()\n",
       "\n",
       ":param async_req bool\n",
       ":param str name: name of the PodExecOptions (required)\n",
       ":param str namespace: object name and auth scope, such as for teams and projects (required)\n",
       ":param str command: Command is the remote command to execute. argv array. Not executed within a shell.\n",
       ":param str container: Container in which to execute the command. Defaults to only container if there is only one container in the pod.\n",
       ":param bool stderr: Redirect the standard error stream of the pod for this call. Defaults to true.\n",
       ":param bool stdin: Redirect the standard input stream of the pod for this call. Defaults to false.\n",
       ":param bool stdout: Redirect the standard output stream of the pod for this call. Defaults to true.\n",
       ":param bool tty: TTY if true indicates that a tty will be allocated for the exec call. Defaults to false.\n",
       ":return: str\n",
       "         If the method is called asynchronously,\n",
       "         returns the request thread.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Programs/conda/lib/python3.7/site-packages/kubernetes_asyncio/client/api/core_v1_api.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api = kube.client.CoreV1Api()\n",
    "api.connect_get_namespaced_pod_exec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing inside a container\n",
    "\n",
    "Example: <https://github.com/tomplus/kubernetes_asyncio/blob/master/examples/example3.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/bin/timeout', '5', '/usr/bin/nvidia-smi', '--format=csv', '--loop=1', '--query-gpu=index,utilization.gpu,utilization.memory,memory.used,memory.total']\n",
      "Req <coroutine object ApiClient.__call_api at 0x7f7c278533b0>\n",
      "Query response:  index, utilization.gpu [%], utilization.memory [%], memory.used [MiB], memory.total [MiB]\n",
      "0, 99 %, 60 %, 17613 MiB, 32510 MiB\n",
      "1, 100 %, 60 %, 17547 MiB, 32510 MiB\n",
      "0, 99 %, 58 %, 17613 MiB, 32510 MiB\n",
      "1, 100 %, 56 %, 17547 MiB, 32510 MiB\n",
      "0, 99 %, 66 %, 17613 MiB, 32510 MiB\n",
      "1, 100 %, 65 %, 17547 MiB, 32510 MiB\n",
      "0, 99 %, 60 %, 17613 MiB, 32510 MiB\n",
      "1, 100 %, 59 %, 17547 MiB, 32510 MiB\n",
      "0, 100 %, 65 %, 17613 MiB, 32510 MiB\n",
      "1, 100 %, 67 %, 17547 MiB, 32510 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPU_QUERY_FIELDS = [\n",
    "\t'index',\n",
    "\t'utilization.gpu',\n",
    "\t'utilization.memory',\n",
    "\t'memory.used',\n",
    "\t'memory.total',\n",
    "]\n",
    "GPU_QUERY_CMD = [\n",
    "\t'/usr/bin/timeout', '5',\n",
    "\t'/usr/bin/nvidia-smi',\n",
    "\t'--format=csv',\n",
    "  \t'--loop=1',\n",
    "\tf'--query-gpu={\",\".join(GPU_QUERY_FIELDS)}',\n",
    "]\n",
    "print(GPU_QUERY_CMD)\n",
    "\n",
    "async def query_gpu_status():\n",
    "\tawait kube.config.load_kube_config()\n",
    "\tapi_ws = kube.client.CoreV1Api(api_client=kube.stream.WsApiClient())\n",
    "# \tapi_ws = kube.client.CoreV1Api()\n",
    "\n",
    "\tcmd = GPU_QUERY_CMD\n",
    "\tname = 'sguo-pytorch-imagenet-iclr'\n",
    "# \tname = 'wwang-imagenet2'\n",
    "\tnamespace = 'cvlab'\n",
    "\t\n",
    "# \tcmd = ['ls']\n",
    "\t\n",
    "\treq = api_ws.connect_get_namespaced_pod_exec(\n",
    "\t\tname = name, \n",
    "\t\tnamespace = namespace,\n",
    "\t\tcommand = cmd,\n",
    "\t\tstderr=True,\n",
    "\t\tstdin=False,\n",
    "\t\tstdout=True,\n",
    "\t\ttty=False,\n",
    "\n",
    "# \t\tasync_req=True,\n",
    "# \t\t_preload_content=False,\n",
    "\t)\n",
    "\t\n",
    "\tprint('Req', req)\n",
    "\tresponse = await req\n",
    "\tprint(\"Query response: \", response)\n",
    "\treturn response\n",
    "\t\n",
    "# \tw = kube.watch.Watch()\n",
    "\t\n",
    "# \tasync for event in w.stream(api_ws.connect_get_namespaced_pod_exec,\n",
    "# \t\tname = name, \n",
    "# \t\tnamespace = namespace,\n",
    "# \t\tcommand = cmd,\n",
    "# \t\tstderr=True,\n",
    "# \t\tstdin=False,\n",
    "# \t\tstdout=True,\n",
    "# \t\ttty=False,\n",
    "\t\t\t\t\t\t\t\t\n",
    "# \t\tasync_req=True,\n",
    "# \t\t_preload_content=False,\n",
    "# \t):\n",
    "# \t\tprint(event)\n",
    "\t\n",
    "\n",
    "# \treturn response\n",
    "\n",
    "r = await query_gpu_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['index',\n",
       "  ' utilization.gpu [%]',\n",
       "  ' utilization.memory [%]',\n",
       "  ' memory.used [MiB]',\n",
       "  ' memory.total [MiB]'],\n",
       " ['0', ' 99 %', ' 60 %', ' 17613 MiB', ' 32510 MiB'],\n",
       " ['1', ' 100 %', ' 60 %', ' 17547 MiB', ' 32510 MiB'],\n",
       " ['0', ' 99 %', ' 58 %', ' 17613 MiB', ' 32510 MiB'],\n",
       " ['1', ' 100 %', ' 56 %', ' 17547 MiB', ' 32510 MiB'],\n",
       " ['0', ' 99 %', ' 66 %', ' 17613 MiB', ' 32510 MiB'],\n",
       " ['1', ' 100 %', ' 65 %', ' 17547 MiB', ' 32510 MiB'],\n",
       " ['0', ' 99 %', ' 60 %', ' 17613 MiB', ' 32510 MiB'],\n",
       " ['1', ' 100 %', ' 59 %', ' 17547 MiB', ' 32510 MiB'],\n",
       " ['0', ' 100 %', ' 65 %', ' 17613 MiB', ' 32510 MiB'],\n",
       " ['1', ' 100 %', ' 67 %', ' 17547 MiB', ' 32510 MiB'],\n",
       " []]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(csv.reader(r.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
